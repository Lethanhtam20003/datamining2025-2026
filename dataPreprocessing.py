# ================= DATA PREPROCESSING MODULE =================
# 1. L√†m s·∫°ch d·ªØ li·ªáu vƒÉn b·∫£n
# 2. Chu·∫©n h√≥a ti·∫øng Vi·ªát
# 3. X·ª≠ l√Ω slang (ng√¥n ng·ªØ chat)
# 4. Tokenize ti·∫øng Vi·ªát
# 5. Lo·∫°i b·ªè stopwords
# 6. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng emoji
# 7. Vector h√≥a vƒÉn b·∫£n b·∫±ng TF-IDF

import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from underthesea import word_tokenize
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

# ================= STOPWORDS TI·∫æNG VI·ªÜT =================
with open('stopWords_vietnamese.txt', 'r', encoding='utf-8') as f:
    STOPWORDS = set(line.strip() for line in f if line.strip())

# ================= EMOJI GROUPS =================
POS_EMOJIS = [
    '‚ù§Ô∏è', '‚ù§', '‚ô•Ô∏è', 'ü•∞', 'üòç', 'üòä', 'üéâ', 'üî•', 'ü§£', 'üòÇ', 'üòÖ', 'üëç', 'üëè',
    '‚ú®', 'üåπ', 'ü§©', 'üéä', 'üéÜ', 'üéá', 'üçÄ', '‚òòÔ∏è', 'üåü', 'üòÑ', 'üòÅ', 'üôå', 'üëå',
    'üíê', 'üå∏', 'üé§', 'üéµ', 'üé∂', 'üíô', 'üíó', 'üíû', 'üíï', 'üíñ', 'üíì'
]
NEG_EMOJIS = ['üò¢', 'üò≠', 'üòû', 'üòî', 'ü•∫', 'üíî', 'üëé', 'üò°', 'üò†', 'üò§', 'üí¢', 'üòí', 'üôÑ']
NEU_EMOJIS = ['üòê', 'üò∂', 'ü§î', 'üßê', 'üò¨', 'üòë', 'ü§´']

# ================= SLANG DICTIONARY =================
SLANG_DICT = {
    # Slang ti·∫øng Vi·ªát
    "k": "kh√¥ng", "ko": "kh√¥ng", "kh": "kh√¥ng", "v": "v·∫≠y",
    "ƒëc": "ƒë∆∞·ª£c", "dc": "ƒë∆∞·ª£c", "r": "r·ªìi", "s": "sao",
    "mn": "m·ªçi ng∆∞·ªùi", "ae": "anh em", "m√¨h": "m√¨nh", "mik": "m√¨nh",
    "tr": "tr·ªùi", "j": "g√¨", "bt": "bi·∫øt", "kb": "kh√¥ng bi·∫øt", "h": "gi·ªù",
    # T·ª´ ti·∫øng Anh bi·ªÉu c·∫£m -> Ti·∫øng Vi·ªát
    "good": "t·ªët", "great": "tuy·ªát", "awesome": "tuy·ªát_v·ªùi", "excellent": "xu·∫•t_s·∫Øc",
    "bad": "t·ªá", "sucks": "t·ªá", "terrible": "t·ªìi_t·ªá", "worst": "t·ªìi_t·ªá",
    "like": "th√≠ch", "love": "y√™u", "hate": "gh√©t", "dislike": "kh√¥ng_th√≠ch",
    "nice": "t·ªët", "cool": "ng·∫ßu", "chill": "tho·∫£i_m√°i", "fun": "vui",
    "boring": "nh√†m_ch√°n", "sad": "bu·ªìn", "happy": "vui", "excited": "h√†o_h·ª©ng",
    "amazing": "kinh_ng·∫°c", "wonderful": "tuy·ªát_v·ªùi", "fantastic": "tuy·ªát_v·ªùi",
    "horrible": "kinh_kh·ªßng", "awful": "t·ªìi_t·ªá", "disgusting": "gh√™_t·ªüm",
    "perfect": "ho√†n_h·∫£o", "super": "si√™u", "best": "t·ªët_nh·∫•t",
    "okay": "·ªïn", "ok": "·ªïn", "fine": "t·ªët", "alright": "·ªïn",
    "shit": "t·ªá", "damn": "ch·∫øt_ti·ªát", "fuck": "ch·∫øt_ti·ªát"
}


def load_data(file_path):
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
    Lo·∫°i b·ªè c√°c d√≤ng b·ªã thi·∫øu n·ªôi dung vƒÉn b·∫£n
    """
    df = pd.read_csv(file_path)
    df.dropna(subset=['Text'], inplace=True)
    return df

def clean_text(text):
    """
    L√†m s·∫°ch vƒÉn b·∫£n:
    - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    - Lo·∫°i b·ªè URL
    - Lo·∫°i b·ªè th·∫ª HTML
    - Gi·ªØ l·∫°i ch·ªØ c√°i, ch·ªØ s·ªë v√† kho·∫£ng tr·∫Øng
    """
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(
        r"[^a-zA-Z0-9√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ√Ω\s]",
        '',
        text
    )
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_emoji(text, emoji_list):
    """
    Tr√≠ch xu·∫•t emoji thu·ªôc m·ªôt nh√≥m nh·∫•t ƒë·ªãnh t·ª´ vƒÉn b·∫£n
    D√πng ƒë·ªÉ t·∫°o feature: emoji_pos, emoji_neg, emoji_neu
    """
    if not isinstance(text, str):
        return ""
    return "".join([c for c in text if c in emoji_list])

def convert_slang(text):
    """
    Chuy·ªÉn ƒë·ªïi slang (ng√¥n ng·ªØ chat) sang ti·∫øng Vi·ªát chu·∫©n
    V√≠ d·ª•: 'ko bt j' -> 'kh√¥ng bi·∫øt g√¨'
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    return " ".join([SLANG_DICT.get(w, w) for w in words])

def tokenize_vietnamese(text):
    """
    T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng th∆∞ vi·ªán underthesea
    Tr·∫£ v·ªÅ list c√°c t·ª´ ƒë√£ t√°ch
    """
    if not isinstance(text, str):
        return []
    return word_tokenize(text, format="text").split()


def negation_transformation(word_list):
    """
    Th·ª±c hi·ªán Negation Transformation: gh√©p t·ª´ ph·ªß ƒë·ªãnh v·ªõi t·ª´ ƒë·ª©ng sau
    V√≠ d·ª•: ["kh√¥ng", "h√†i_l√≤ng"] -> ["kh√¥ng_h√†i_l√≤ng"]
    """
    if not word_list:
        return word_list
    
    negation_words = ["kh√¥ng", "ch∆∞a", "ch·∫≥ng", "ch·∫£", "ch·ªõ", "ƒë·ª´ng", "kh√¥ng_th√≠ch", "ch∆∞a_bao_gi·ªù"]
    transformed = []
    i = 0
    while i < len(word_list):
        if i < len(word_list) - 1 and word_list[i] in negation_words:
            # Gh√©p negation v·ªõi t·ª´ ti·∫øp theo
            transformed.append(word_list[i] + "_" + word_list[i+1])
            i += 2  # B·ªè qua t·ª´ ti·∫øp theo v√¨ ƒë√£ gh√©p
        else:
            transformed.append(word_list[i])
            i += 1
    return transformed

def remove_stopwords(text):
    """
    Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát kh·ªèi vƒÉn b·∫£n
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    words = [w for w in words if w not in STOPWORDS]
    return " ".join(words)

# ================= LABELING HEURISTIC =================
POS_WORDS = ["t·ªët", "hay", "th√≠ch", "tuy·ªát", "love", "good", "nice", "excellent", "tuy·ªát_v·ªùi", "∆∞ng", "ok", "oki", "tuy·ªát", "h√†i_l√≤ng", "∆∞ng_√Ω"]
NEG_WORDS = ["t·ªá", "d·ªü", "gh√©t", "k√©m", "bad", "hate", "worst", "terrible", "ch√°n", "bu·ªìn", "kh√¥ng_th√≠ch", "t·ªìi_t·ªá", "kh√¥ng_h√†i_l√≤ng"]

def assign_label(row):
    """
    G√°n nh√£n d·ª±a tr√™n heuristic: emoji > t·ª´ kh√≥a + x·ª≠ l√Ω ph·ªß ƒë·ªãnh
    1 = Khen, 0 = Ch√™, 2 = Trung t√≠nh
    """
    # ∆Øu ti√™n emoji
    if row['num_emoji_pos'] > 0:
        return 1  # Khen
    elif row['num_emoji_neg'] > 0:
        return 0  # Ch√™
    elif row['num_emoji_neu'] > 0:
        return 2  # Trung t√≠nh
    
    # N·∫øu kh√¥ng c√≥ emoji, ki·ªÉm tra t·ª´ kh√≥a v·ªõi x·ª≠ l√Ω ph·ªß ƒë·ªãnh
    text = str(row['cleaned_text']).lower()
    
    # T√°ch t·ª´
    words = text.split()
    
    # Ki·ªÉm tra bigram ph·ªß ƒë·ªãnh
    negation_words = ["kh√¥ng", "kh√¥ng_th√≠ch", "ch·∫≥ng", "ch∆∞a", "ko"]
    pos_words = ["t·ªët", "hay", "th√≠ch", "tuy·ªát", "tuy·ªát_v·ªùi", "∆∞ng", "·ªïn", "xu·∫•t_s·∫Øc", "ho√†n_h·∫£o"]
    neg_words = ["t·ªá", "d·ªü", "gh√©t", "k√©m", "t·ªìi_t·ªá", "nh√†m_ch√°n", "bu·ªìn", "gh√™_t·ªüm"]
    
    # Ki·ªÉm tra ph·ªß ƒë·ªãnh + t·ª´ t√≠ch c·ª±c = ti√™u c·ª±c
    for i in range(len(words) - 1):
        if words[i] in negation_words and words[i+1] in pos_words:
            return 0  # Ch√™ (kh√¥ng t·ªët = t·ªá)
        if words[i] in negation_words and words[i+1] in neg_words:
            return 1  # Khen (kh√¥ng t·ªá = t·ªët)
    
    # Ki·ªÉm tra t·ª´ kh√≥a ƒë∆°n gi·∫£n
    if any(word in pos_words for word in words):
        return 1
    elif any(word in neg_words for word in words):
        return 0
    else:
        return 2  # M·∫∑c ƒë·ªãnh trung t√≠nh

# ================= MAIN =================
if __name__ == "__main__":
    df = load_data("data/comments.csv")

    # 1. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng Emoji (Ph·∫£i l√†m TR∆Ø·ªöC khi clean_text x√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát)
    df['emoji_pos'] = df['Text'].apply(lambda x: extract_emoji(x, POS_EMOJIS))
    df['emoji_neg'] = df['Text'].apply(lambda x: extract_emoji(x, NEG_EMOJIS))
    df['emoji_neu'] = df['Text'].apply(lambda x: extract_emoji(x, NEU_EMOJIS))
    
    # 2. T√≠nh to√°n ƒë·∫∑c tr∆∞ng s·ªë (Numeric Features)
    df['num_emoji_pos'] = df['emoji_pos'].apply(len)
    df['num_emoji_neg'] = df['emoji_neg'].apply(len)
    df['num_emoji_neu'] = df['emoji_neu'].apply(len)

    # 3. Pipeline Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (Th·ª© t·ª± t·ªëi ∆∞u)
    # B∆∞·ªõc a: X·ª≠ l√Ω slang tr∆∞·ªõc ƒë·ªÉ chu·∫©n h√≥a t·ª´ ng·ªØ cho underthesea
    df['cleaned_text'] = df['Text'].apply(convert_slang)
    # B∆∞·ªõc b: L√†m s·∫°ch (X√≥a URL, HTML, k√Ω t·ª± ƒë·∫∑c bi·ªát...)
    df['cleaned_text'] = df['cleaned_text'].apply(clean_text)
    # B∆∞·ªõc c: T√°ch t·ª´ ti·∫øng Vi·ªát 
    df['tokenized'] = df['cleaned_text'].apply(tokenize_vietnamese)
    # B∆∞·ªõc d: X·ª≠ l√Ω ph·ªß ƒë·ªãnh
    df['tokenized'] = df['tokenized'].apply(negation_transformation)
    # B∆∞·ªõc e: Gh√©p l·∫°i v√† lo·∫°i b·ªè Stopwords
    df['cleaned_text'] = df['tokenized'].apply(lambda x: " ".join(x))
    # df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)  # T·∫°m b·ªè ƒë·ªÉ test

    # 4. G√°n nh√£n (Labeling)
    df['label'] = df.apply(assign_label, axis=1)
    # L·ªçc b·ªè samples tr·ªëng sau preprocessing
    df = df[df['cleaned_text'].str.len() > 0].copy()
    # 5. Vector h√≥a vƒÉn b·∫£n b·∫±ng TF-IDF
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=1, max_df=1.0)
    X_tfidf = tfidf.fit_transform(df['cleaned_text']).toarray()

    # 6. CHU·∫®N H√ìA (NORMALIZATION) - B∆∞·ªõc t·ªëi ∆∞u quan tr·ªçng 
    # ƒê∆∞a c√°c c·ªôt s·ªë l∆∞·ª£ng emoji v·ªÅ c√πng thang ƒëo [0, 1] nh∆∞ TF-IDF
    scaler = MinMaxScaler()
    emoji_numeric = df[['num_emoji_pos', 'num_emoji_neg', 'num_emoji_neu']].values
    emoji_scaled = scaler.fit_transform(emoji_numeric)

    # 7. T√çCH H·ª¢P D·ªÆ LI·ªÜU (Data Integration) 
    # K·∫øt h·ª£p vector t·ª´ v·ª±ng v√† vector emoji ƒë√£ chu·∫©n h√≥a
    X_final = np.hstack([X_tfidf, emoji_scaled])

    print("K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng cu·ªëi c√πng:", X_final.shape)
    print("Ph√¢n b·ªë nh√£n:", df['label'].value_counts())
    df.to_csv("data/comments_final_optimized.csv", index=False, encoding='utf-8-sig')    # ================= LABELING HEURISTIC =================
    POS_WORDS = ["t·ªët", "hay", "th√≠ch", "tuy·ªát", "love", "good", "nice", "excellent", "tuy·ªát_v·ªùi", "∆∞ng", "ok", "oki"]
    NEG_WORDS = ["t·ªá", "d·ªü", "gh√©t", "k√©m", "bad", "hate", "worst", "terrible", "ch√°n", "bu·ªìn", "kh√¥ng_th√≠ch"]
    
    def assign_label(row):
        """
        G√°n nh√£n d·ª±a tr√™n heuristic: emoji > t·ª´ kh√≥a
        """
        # ∆Øu ti√™n emoji
        if row['num_emoji_pos'] > 0:
            return 1  # Khen
        elif row['num_emoji_neg'] > 0:
            return 0  # Ch√™
        elif row['num_emoji_neu'] > 0:
            return 2  # Trung t√≠nh
        
        # N·∫øu kh√¥ng c√≥ emoji, ki·ªÉm tra t·ª´ kh√≥a
        text = row['cleaned_text'].lower()
        if any(word in text for word in POS_WORDS):
            return 1
        elif any(word in text for word in NEG_WORDS):
            return 0
        else:
            return 2  # M·∫∑c ƒë·ªãnh trung t√≠nh
    
    # Th√™m v√†o pipeline ch√≠nh:
    df['label'] = df.apply(assign_label, axis=1)    
    
    # Sau khi c√≥ nh√£n v√† X_final
    X_train, X_test, y_train, y_test = train_test_split(
        X_final, df['label'], test_size=0.2, random_state=42, stratify=df['label']
    )
    print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")

    # Hu·∫•n luy·ªán
    nb_model = MultinomialNB(alpha=1.0)  # Laplace smoothing
    nb_model.fit(X_train, y_train)

    # D·ª± ƒëo√°n
    y_pred = nb_model.predict(X_test)

    # ƒê√°nh gi√° c∆° b·∫£n
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Ch√™ (0)', 'Khen (1)', 'Trung t√≠nh (2)']))