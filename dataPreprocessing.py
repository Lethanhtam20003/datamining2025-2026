# ================= DATA PREPROCESSING MODULE =================
# 1. LÃ m sáº¡ch dá»¯ liá»‡u vÄƒn báº£n
# 2. Chuáº©n hÃ³a tiáº¿ng Viá»‡t
# 3. Xá»­ lÃ½ slang (ngÃ´n ngá»¯ chat)
# 4. Tokenize tiáº¿ng Viá»‡t
# 5. Loáº¡i bá» stopwords
# 6. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng emoji
# 7. Vector hÃ³a vÄƒn báº£n báº±ng TF-IDF

import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from underthesea import word_tokenize
import emoji
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

# ================= STOPWORDS TIáº¾NG VIá»†T =================
with open('stopWords_vietnamese.txt', 'r', encoding='utf-8') as f:
    STOPWORDS = set(line.strip() for line in f if line.strip())

# ================= EMOJI GROUPS =================
POS_EMOJIS = [
    'â¤ï¸', 'â¤', 'â™¥ï¸', 'ğŸ¥°', 'ğŸ˜', 'ğŸ˜Š', 'ğŸ‰', 'ğŸ”¥', 'ğŸ¤£', 'ğŸ˜‚', 'ğŸ˜…', 'ğŸ‘', 'ğŸ‘',
    'âœ¨', 'ğŸŒ¹', 'ğŸ¤©', 'ğŸŠ', 'ğŸ†', 'ğŸ‡', 'ğŸ€', 'â˜˜ï¸', 'ğŸŒŸ', 'ğŸ˜„', 'ğŸ˜', 'ğŸ™Œ', 'ğŸ‘Œ',
    'ğŸ’', 'ğŸŒ¸', 'ğŸ¤', 'ğŸµ', 'ğŸ¶', 'ğŸ’™', 'ğŸ’—', 'ğŸ’', 'ğŸ’•', 'ğŸ’–', 'ğŸ’“'
]
NEG_EMOJIS = ['ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜', 'ğŸ˜”', 'ğŸ¥º', 'ğŸ’”', 'ğŸ‘', 'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ˜¤', 'ğŸ’¢', 'ğŸ˜’', 'ğŸ™„']
NEU_EMOJIS = ['ğŸ˜', 'ğŸ˜¶', 'ğŸ¤”', 'ğŸ§', 'ğŸ˜¬', 'ğŸ˜‘', 'ğŸ¤«']

# ================= SLANG DICTIONARY =================
SLANG_DICT = {
    "k": "khÃ´ng", "ko": "khÃ´ng", "kh": "khÃ´ng", "v": "váº­y",
    "Ä‘c": "Ä‘Æ°á»£c", "dc": "Ä‘Æ°á»£c", "r": "rá»“i", "s": "sao",
    "mn": "má»i ngÆ°á»i", "ae": "anh em", "mÃ¬h": "mÃ¬nh", "mik": "mÃ¬nh",
    "tr": "trá»i", "j": "gÃ¬", "bt": "biáº¿t", "kb": "khÃ´ng biáº¿t", "h": "giá»"
}


def load_data(file_path):
    """
    Äá»c dá»¯ liá»‡u tá»« file CSV
    Loáº¡i bá» cÃ¡c dÃ²ng bá»‹ thiáº¿u ná»™i dung vÄƒn báº£n
    """
    df = pd.read_csv(file_path)
    df.dropna(subset=['Text'], inplace=True)
    return df

def clean_text(text):
    """
    LÃ m sáº¡ch vÄƒn báº£n:
    - Chuyá»ƒn vá» chá»¯ thÆ°á»ng
    - Loáº¡i bá» URL
    - Loáº¡i bá» tháº» HTML
    - Giá»¯ láº¡i chá»¯ cÃ¡i, chá»¯ sá»‘ vÃ  khoáº£ng tráº¯ng
    """
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(
        r"[^a-zA-Z0-9Ã Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½\s]",
        '',
        text
    )
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_emoji(text, emoji_list):
    """
    TrÃ­ch xuáº¥t emoji thuá»™c má»™t nhÃ³m nháº¥t Ä‘á»‹nh tá»« vÄƒn báº£n
    DÃ¹ng Ä‘á»ƒ táº¡o feature: emoji_pos, emoji_neg, emoji_neu
    """
    if not isinstance(text, str):
        return ""
    return "".join([c for c in text if c in emoji_list])

def convert_slang(text):
    """
    Chuyá»ƒn Ä‘á»•i slang (ngÃ´n ngá»¯ chat) sang tiáº¿ng Viá»‡t chuáº©n
    VÃ­ dá»¥: 'ko bt j' -> 'khÃ´ng biáº¿t gÃ¬'
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    return " ".join([SLANG_DICT.get(w, w) for w in words])

def tokenize_vietnamese(text):
    """
    TÃ¡ch tá»« tiáº¿ng Viá»‡t báº±ng thÆ° viá»‡n underthesea
    VÃ­ dá»¥: 'ráº¥t thÃ­ch sáº£n pháº©m' -> 'ráº¥t_thÃ­ch sáº£n_pháº©m'
    """
    if not isinstance(text, str):
        return ""
    return word_tokenize(text, format="text")

def remove_stopwords(text):
    """
    Loáº¡i bá» stopwords tiáº¿ng Viá»‡t khá»i vÄƒn báº£n
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    words = [w for w in words if w not in STOPWORDS]
    return " ".join(words)

# ================= MAIN =================
if __name__ == "__main__":
    df = load_data("comments.csv")

    # 1. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng Emoji (Pháº£i lÃ m TRÆ¯á»šC khi clean_text xÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t)
    df['emoji_pos'] = df['Text'].apply(lambda x: extract_emoji(x, POS_EMOJIS))
    df['emoji_neg'] = df['Text'].apply(lambda x: extract_emoji(x, NEG_EMOJIS))
    df['emoji_neu'] = df['Text'].apply(lambda x: extract_emoji(x, NEU_EMOJIS))
    
    # 2. TÃ­nh toÃ¡n Ä‘áº·c trÆ°ng sá»‘ (Numeric Features)
    df['num_emoji_pos'] = df['emoji_pos'].apply(len)
    df['num_emoji_neg'] = df['emoji_neg'].apply(len)
    df['num_emoji_neu'] = df['emoji_neu'].apply(len)

    # 3. Pipeline Tiá»n xá»­ lÃ½ vÄƒn báº£n (Thá»© tá»± tá»‘i Æ°u)
    # BÆ°á»›c a: Xá»­ lÃ½ slang trÆ°á»›c Ä‘á»ƒ chuáº©n hÃ³a tá»« ngá»¯ cho underthesea
    df['cleaned_text'] = df['Text'].apply(convert_slang)
    # BÆ°á»›c b: LÃ m sáº¡ch (XÃ³a URL, HTML, kÃ½ tá»± Ä‘áº·c biá»‡t...)
    df['cleaned_text'] = df['cleaned_text'].apply(clean_text)
    # BÆ°á»›c c: TÃ¡ch tá»« tiáº¿ng Viá»‡t 
    df['cleaned_text'] = df['cleaned_text'].apply(tokenize_vietnamese)
    # BÆ°á»›c d: Loáº¡i bá» Stopwords
    df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)

    # 4. Vector hÃ³a vÄƒn báº£n báº±ng TF-IDF
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.9)
    X_tfidf = tfidf.fit_transform(df['cleaned_text']).toarray()

    # 5. CHUáº¨N HÃ“A (NORMALIZATION) - BÆ°á»›c tá»‘i Æ°u quan trá»ng 
    # ÄÆ°a cÃ¡c cá»™t sá»‘ lÆ°á»£ng emoji vá» cÃ¹ng thang Ä‘o [0, 1] nhÆ° TF-IDF
    scaler = MinMaxScaler()
    emoji_numeric = df[['num_emoji_pos', 'num_emoji_neg', 'num_emoji_neu']].values
    emoji_scaled = scaler.fit_transform(emoji_numeric)

    # 6. TÃCH Há»¢P Dá»® LIá»†U (Data Integration) 
    # Káº¿t há»£p vector tá»« vá»±ng vÃ  vector emoji Ä‘Ã£ chuáº©n hÃ³a
    X_final = np.hstack([X_tfidf, emoji_scaled])

    print("KÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng cuá»‘i cÃ¹ng:", X_final.shape)
    df.to_csv("comments_final_optimized.csv", index=False, encoding='utf-8-sig')    # ================= LABELING HEURISTIC =================
    POS_WORDS = ["tá»‘t", "hay", "thÃ­ch", "tuyá»‡t", "love", "good", "nice", "excellent", "tuyá»‡t_vá»i", "Æ°ng", "ok", "oki"]
    NEG_WORDS = ["tá»‡", "dá»Ÿ", "ghÃ©t", "kÃ©m", "bad", "hate", "worst", "terrible", "chÃ¡n", "buá»“n", "khÃ´ng_thÃ­ch"]
    
    def assign_label(row):
        """
        GÃ¡n nhÃ£n dá»±a trÃªn heuristic: emoji > tá»« khÃ³a
        """
        # Æ¯u tiÃªn emoji
        if row['num_emoji_pos'] > 0:
            return 1  # Khen
        elif row['num_emoji_neg'] > 0:
            return 0  # ChÃª
        elif row['num_emoji_neu'] > 0:
            return 2  # Trung tÃ­nh
        
        # Náº¿u khÃ´ng cÃ³ emoji, kiá»ƒm tra tá»« khÃ³a
        text = row['cleaned_text'].lower()
        if any(word in text for word in POS_WORDS):
            return 1
        elif any(word in text for word in NEG_WORDS):
            return 0
        else:
            return 2  # Máº·c Ä‘á»‹nh trung tÃ­nh
    
    # ThÃªm vÃ o pipeline chÃ­nh:
    df['label'] = df.apply(assign_label, axis=1)    
    
    # Sau khi cÃ³ nhÃ£n vÃ  X_final
    X_train, X_test, y_train, y_test = train_test_split(
        X_final, df['label'], test_size=0.2, random_state=42, stratify=df['label']
    )
    print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")

    # Huáº¥n luyá»‡n
    nb_model = MultinomialNB(alpha=1.0)  # Laplace smoothing
    nb_model.fit(X_train, y_train)

    # Dá»± Ä‘oÃ¡n
    y_pred = nb_model.predict(X_test)

    # ÄÃ¡nh giÃ¡ cÆ¡ báº£n
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['ChÃª (0)', 'Khen (1)', 'Trung tÃ­nh (2)']))