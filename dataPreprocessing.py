# ================= DATA PREPROCESSING MODULE =================
# 1. L√†m s·∫°ch d·ªØ li·ªáu vƒÉn b·∫£n
# 2. Chu·∫©n h√≥a ti·∫øng Vi·ªát
# 3. X·ª≠ l√Ω slang (ng√¥n ng·ªØ chat)
# 4. Tokenize ti·∫øng Vi·ªát
# 5. Lo·∫°i b·ªè stopwords
# 6. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng emoji
# 7. Vector h√≥a vƒÉn b·∫£n b·∫±ng TF-IDF

import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from underthesea import word_tokenize


# ================= STOPWORDS TI·∫æNG VI·ªÜT =================
STOPWORDS = set([
    "l√†", "c·ªßa", "v√†", "nh∆∞ng", "ƒë√£", "ƒëang", "s·∫Ω", "c≈©ng", "cho", "r·∫±ng",
    "nh·ªØng", "c√°i", "con", "th√¨", "m√†", "l·∫°i", "v·ªõi", "t·∫°i", "n√†y", "v·∫≠y", "∆°i", "·∫°"
])

# ================= EMOJI GROUPS =================
POS_EMOJIS = [
    '‚ù§Ô∏è', '‚ù§', '‚ô•Ô∏è', 'ü•∞', 'üòç', 'üòä', 'üéâ', 'üî•', 'ü§£', 'üòÇ', 'üòÖ', 'üëç', 'üëè',
    '‚ú®', 'üåπ', 'ü§©', 'üéä', 'üéÜ', 'üéá', 'üçÄ', '‚òòÔ∏è', 'üåü', 'üòÑ', 'üòÅ', 'üôå', 'üëå',
    'üíê', 'üå∏', 'üé§', 'üéµ', 'üé∂', 'üíô', 'üíó', 'üíû', 'üíï', 'üíñ', 'üíì'
]
NEG_EMOJIS = ['üò¢', 'üò≠', 'üòû', 'üòî', 'ü•∫', 'üíî', 'üëé', 'üò°', 'üò†', 'üò§', 'üí¢', 'üòí', 'üôÑ']
NEU_EMOJIS = ['üòê', 'üò∂', 'ü§î', 'üßê', 'üò¨', 'üòë', 'ü§´']

# ================= SLANG DICTIONARY =================
SLANG_DICT = {
    "k": "kh√¥ng", "ko": "kh√¥ng", "kh": "kh√¥ng", "v": "v·∫≠y",
    "ƒëc": "ƒë∆∞·ª£c", "dc": "ƒë∆∞·ª£c", "r": "r·ªìi", "s": "sao",
    "mn": "m·ªçi ng∆∞·ªùi", "ae": "anh em", "m√¨h": "m√¨nh", "mik": "m√¨nh",
    "tr": "tr·ªùi", "j": "g√¨", "bt": "bi·∫øt", "kb": "kh√¥ng bi·∫øt", "h": "gi·ªù"
}


def load_data(file_path):
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV
    Lo·∫°i b·ªè c√°c d√≤ng b·ªã thi·∫øu n·ªôi dung vƒÉn b·∫£n
    """
    df = pd.read_csv(file_path)
    df.dropna(subset=['Text'], inplace=True)
    return df


def extract_emoji(text, emoji_list):
    """
    Tr√≠ch xu·∫•t emoji thu·ªôc m·ªôt nh√≥m nh·∫•t ƒë·ªãnh t·ª´ vƒÉn b·∫£n
    D√πng ƒë·ªÉ t·∫°o feature: emoji_pos, emoji_neg, emoji_neu
    """
    if not isinstance(text, str):
        return ""
    return "".join([c for c in text if c in emoji_list])


def convert_slang(text):
    """
    Chuy·ªÉn ƒë·ªïi slang (ng√¥n ng·ªØ chat) sang ti·∫øng Vi·ªát chu·∫©n
    V√≠ d·ª•: 'ko bt j' -> 'kh√¥ng bi·∫øt g√¨'
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    return " ".join([SLANG_DICT.get(w, w) for w in words])


def clean_text(text):
    """
    L√†m s·∫°ch vƒÉn b·∫£n:
    - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    - Lo·∫°i b·ªè URL
    - Lo·∫°i b·ªè th·∫ª HTML
    - Gi·ªØ l·∫°i ch·ªØ c√°i, ch·ªØ s·ªë v√† kho·∫£ng tr·∫Øng
    """
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(
        r"[^a-zA-Z0-9√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ√Ω\s]",
        '',
        text
    )
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def tokenize_vietnamese(text):
    """
    T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng th∆∞ vi·ªán underthesea
    V√≠ d·ª•: 'r·∫•t th√≠ch s·∫£n ph·∫©m' -> 'r·∫•t_th√≠ch s·∫£n_ph·∫©m'
    """
    if not isinstance(text, str):
        return ""
    return word_tokenize(text, format="text")


def remove_stopwords(text):
    """
    Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát kh·ªèi vƒÉn b·∫£n
    """
    if not isinstance(text, str):
        return ""
    words = text.split()
    words = [w for w in words if w not in STOPWORDS]
    return " ".join(words)


# ================= MAIN =================
if __name__ == "__main__":
    # Load d·ªØ li·ªáu
    df = load_data("comments.csv")

    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng emoji
    df['emoji_pos'] = df['Text'].apply(lambda x: extract_emoji(x, POS_EMOJIS))
    df['emoji_neg'] = df['Text'].apply(lambda x: extract_emoji(x, NEG_EMOJIS))
    df['emoji_neu'] = df['Text'].apply(lambda x: extract_emoji(x, NEU_EMOJIS))

    # L√†m ph·∫≥ng vƒÉn b·∫£n g·ªëc
    df['Text'] = df['Text'].apply(
        lambda x: x.replace('\n', ' ').replace('\r', ' ') if isinstance(x, str) else x
    )

    # ===== PIPELINE TI·ªÄN X·ª¨ L√ù =====
    df['cleaned_text'] = df['Text'].apply(convert_slang)
    df['cleaned_text'] = df['cleaned_text'].apply(clean_text)
    df['cleaned_text'] = df['cleaned_text'].apply(tokenize_vietnamese)
    df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)

    # ================= TF-IDF =================
    tfidf = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.9
    )

    X_tfidf = tfidf.fit_transform(df['cleaned_text'])
    print("K√≠ch th∆∞·ªõc TF-IDF:", X_tfidf.shape)

    # ================= EMOJI NUMERIC FEATURES =================
    df['num_emoji_pos'] = df['emoji_pos'].apply(len)
    df['num_emoji_neg'] = df['emoji_neg'].apply(len)
    df['num_emoji_neu'] = df['emoji_neu'].apply(len)

    # ================= FINAL FEATURE MATRIX =================
    X_final = np.hstack([
        X_tfidf.toarray(),
        df[['num_emoji_pos', 'num_emoji_neg', 'num_emoji_neu']].values
    ])

    print("K√≠ch th∆∞·ªõc feature cu·ªëi c√πng:", X_final.shape)

    # ================= SAVE FILE =================
    output_name = "comments_final_excel.csv"
    df.to_csv(output_name, index=False, encoding='utf-8-sig')

    print(f"Ho√†n th√†nh File '{output_name}' ƒë√£ s·∫µn s√†ng.")
